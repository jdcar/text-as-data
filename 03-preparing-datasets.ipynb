{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9566fb",
   "metadata": {},
   "source": [
    "<img align=\"left\" width=\"200\" src=\"Picture1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68898835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45243ddf-55b0-4e6d-b829-cbb89b42226d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Using NLP on texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372e8662-49f1-4d84-a139-98e5ef91fb7d",
   "metadata": {},
   "source": [
    "Like any data analysis project, the data might to be prepared before doing Natural Language Processing (NLP). This session will use a NLP tool called spaCy to remove certain words in a document for textual analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c806a-42c2-439c-ac10-ecf24cce005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run the line below if spaCy is not downloaded already.\n",
    "#! python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbea6d2b-103a-49ce-bc9a-740e6edbd8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d475ada0-089a-417e-8831-cc80a56f7936",
   "metadata": {},
   "source": [
    "SpaCy has stopwords, which are words that you might want to remove before doing NLP. The code in the next cell will print the stop words so you can see the list. More advanced NLP projects might create their own stopwords, depending on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd97a577-8e19-4591-861b-d33467a2971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "print(len(stopwords))\n",
    "print(stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68dc61a-496a-47b8-823d-98035abbc74f",
   "metadata": {},
   "source": [
    "I've added a file `little-women.txt` which contains the first chapter of Little Women. Running the code below loads that text and renames it `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479dc168-2f40-49ad-b5b9-77c757e6d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the Little Women text file\n",
    "with open (\"little-women.txt\", \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe913665-097b-43ab-9c34-c39daa4f2c5a",
   "metadata": {},
   "source": [
    "Next, we will print out the first 500 words so we can see how the text looks unprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64ce8d8-e463-4cb5-8d57-1137a904061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd953c7-99aa-473c-94b9-9f20407f7a87",
   "metadata": {},
   "source": [
    "Looks pretty normal right? Now, we're going to use spaCy, which will allow us to take that text and turn it into tokens. When the text becomes tokens, we can use NLP to do textual analysis. After we print the doc as tokens it will look the same, but the computer will understand the text as tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3716d7b-de2e-46ed-8efa-bce6fe01149d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "print (doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130ebe7c-1682-4485-bec9-2a56756b56a8",
   "metadata": {},
   "source": [
    "By running the next code, we see the text is 21,861 characters but when it is processed into tokens, the length becomes 5,473."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c2f202-735d-4e93-8e1a-491f53ea6176",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (len(text))\n",
    "print (len(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bb1f08-55c8-4af6-a85d-63043055c03c",
   "metadata": {},
   "source": [
    "The next cell prints the first 50 tokens on each line. We see that words are their own tokens, as well as punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468b9ff3-3932-48ec-a817-4ff642856ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[:50]:\n",
    "    print (token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd8dc85",
   "metadata": {},
   "source": [
    "Before doing more text analysis it might be good to remove stop words and punctuation: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    if token.is_stop == False: #if the token is not a stop word, keep it\n",
    "        if token.pos_ != \"PUNCT\": #if the token is not punctuation, keep it\n",
    "            print (token.text) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e486955-94fc-4231-9302-f3c7f7f86777",
   "metadata": {},
   "source": [
    "The code in the next cell prints the token and linguistic features associated with that word or punctuation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7b6479-979a-4b3a-91c5-8ac422d27e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc[100:125]:\n",
    "    if token.is_stop == False:\n",
    "        if token.pos_ != \"PUNCT\":\n",
    "            print (token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "                token.shape_, token.is_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09eaa93",
   "metadata": {},
   "source": [
    "What is this telling us? The tags are identifying the parts of speech of the tokens, excluding stop words and punctuation. More information on linguistic features can be found at: https://spacy.io/usage/linguistic-features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c1a956-f9c2-4cf0-a840-e4697c32336e",
   "metadata": {},
   "source": [
    "<p>Take this line: `faces face NOUN NNS nsubj xxxx True False`\n",
    "<p>Faces: the token\n",
    "<p>Face: Lemma, or the base form of the word (how it might be in the dictionary)\n",
    "<p>NOUN: Part of speech\n",
    "<p>NNS: detailed part-of-speech tag.\n",
    "<p>nsubj: Syntactic dependency (the relation between tokens)\n",
    "<p>xxxx: Word shape, like capitalization, punctuation, digits\n",
    "<p>True: It is alphabetical\n",
    "<p>False: Is not on the stop word list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b146822-1b11-40cf-acae-35825699e5b1",
   "metadata": {},
   "source": [
    "At this point we've already done a lot: turned text into tokens, removed stop words and punctuation, and annotated text to show information about the parts of speech. The next notebook we are going to do a little more textual analysis, including visualizations, tables, and comparisons. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5469bd34",
   "metadata": {},
   "source": [
    "## More information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6c767b",
   "metadata": {},
   "source": [
    "<p><a href=\"https://nlp.stanford.edu/\">https://nlp.stanford.edu/</a></p>\n",
    "<p><a href=\"https://spacy.io/\">https://spacy.io/</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d8b2f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
